# AIPI520 Project 1 — FourierRDU Final Notebook

## Overview
This repository contains the final notebook presentation `FourierRDU.ipynb`, which develops an hourly temperature forecasting pipeline for the Raleigh–Durham (RDU) region. The approach combines deterministic seasonal Fourier features with autoregressive lag features and a simple trend proxy (EMA), evaluated under a realistic recursive multi-step forecasting setup.

## Data
- **Source**: `revisedweather.csv` (hourly weather; loaded from [Google Drive](https://drive.google.com/file/d/1F48Sgz7V3q5lHvX_g54GdVc0q3e9U6OT/view?usp=sharing) in notebook.
- **Rows/Cols**: ~409k rows, 19 columns (`dt`, `temp`, `humidity`, `wind_speed`, precipitation, engineered `hourDay`, `dayYear`, etc.).
- **Target**: `y` = next-hour temperature (effectively `temp.shift(-1)`).

## Notebook
- Path: `FourierRDU.ipynb`
- Environment: Google Colab; mounts Drive and reads `revisedweather.csv`.
- Final 14 days (14×24 hours) are held out for testing. Remaining history used for training/validation.

## Methodology
- **Baseline**: Linear Regression with features `[temp, hourDay, dayYear]` and recursive roll-forward prediction.
- **Fourier + Lag Features**: See `feats()` in the notebook.
  - Seasonal terms: daily and yearly cycles with K=3 harmonics each:
    - `sin_daily_{1..3}`, `cos_daily_{1..3}` (period 24)
    - `sin_yearly_{1..3}`, `cos_yearly_{1..3}` (period 8760)
  - Autoregressive lags:
    - Hour lags: 1, 2
    - Day-scale lags: 24, 48, 168 (7 days)
  - Trend proxy: 7-day exponential moving average (EMA)
- **Scaling**:
  - `temp` and all `lag_` features standardized by global train mean/std; Fourier terms are already in [-1, 1].
- **Warm-up**: First 168 rows dropped to remove NA from lag features.
- **Recursive multi-step forecasting**:
  - At each horizon step, the previous predicted temperature replaces `temp` in the next step’s feature vector.
  - EMA is updated from the last prediction; lag columns are rolled using available history.

## Models
- **Ridge Regression** (`alpha=50.0`)
- **XGBoost** (default config in notebook; `reg_lambda` intended to mirror Ridge but was not explicitly set)
- **LightGBM** (default config in notebook; `reg_lambda` intended likewise)

## Validation
- **TimeSeriesSplit** with 10 folds on the pre-test training block, `test_size=14×24` per fold.
- For each fold: fit on the train split and evaluate recursively on the validation split using MSE.

## Results
- **Baseline Linear Regression** (features: `[temp, hourDay, dayYear]`):
  - Test MAE ≈ 10.81, Test MSE ≈ 164.22
- **Ridge + Fourier/Lags/EMA**:
  - Validation MSEs across folds: ~[11.32, 11.75, 10.83, 8.80, 9.46, 7.20, 7.49, 5.20, 9.18, 10.87]
  - Mean Validation MSE ≈ 9.21
  - Final Test MSE ≈ 10.85
- **XGBoost**:
  - Validation MSE unstable across folds; Final Test MSE ≈ 1286.42
- **LightGBM**:
  - Final Test MSE ≈ 431.84

Ridge with Fourier + lag features substantially outperformed tree-based methods under the recursive rollout in this configuration.

## How to Run
1. Open `FourierRDU.ipynb` in Google Colab.
2. Ensure `revisedweather.csv` is present at the expected Drive path (or update the read path in the first data cell).
3. Run all cells top-to-bottom. The notebook will:
   - Engineer features (Fourier, lags, EMA)
   - Train/validate with `TimeSeriesSplit`
   - Fit final models and evaluate on the last 14 days
   - Plot predictions vs truth and display feature importances for Ridge

## Known Issues and Notes
- **EMA recursion**: In `predict()`, `prevEMA` is computed but then overwritten by `X_test['ema'].iloc[i]`. Remove the overwrite and keep the recursively updated `prevEMA` to avoid leakage and to realize EMA’s benefit.
- **Undefined hyperparameter `i`**: `XGB(reg_lambda=i)` and `LGBMRegressor(reg_lambda=i)` reference an undefined variable; set a numeric value (e.g., `reg_lambda=50.0`) or tune explicitly.
- **Validation print**: `mean_xgb_val_mse` was computed from `ridge_val_mses` by mistake; compute from `xgb_val_mses`.
- **Lag column reference**: In `predict()`, derive lag columns from the model’s feature matrix (e.g., from `X_train.columns`) rather than the raw `feats` dataframe to ensure consistency.
- **Unused imports**: `SARIMAX`, `adfuller`, `RandomForestRegressor`, `LabelEncoder` are imported but not used.

## Next Steps
- Tune Ridge `alpha` (e.g., grid over `[1, 5, 10, 25, 50, 100]`) using the provided validation routine.
- Stabilize tree models: set explicit regularization and depth, add early stopping, or adopt direct multi-step strategies to mitigate error compounding.
- Extend features: humidity/pressure interactions, holiday flags, weather regime encodings; ensure no leakage and consistent scaling.

---
For details, see the code and outputs within `FourierRDU.ipynb`.

<!-- THE ABOVE README.md was generated by ChatGPT-5 (low reasoning) through Windsurf -->
